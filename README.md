# gluex_analysis_example

GlueX Analysis Example with (hopefully) complete documentation

Author: Mike McCracken, github: mmccracken0137

## What are we doing here?

We will go through the basic stages of analyzing the $$\gamma p \rightarrow K^{+}\Lambda \rightarrow K^{+}p \pi^{-}$$ reaction.

## Analysis steps

## Accessing the JLab CUE system
ssh to the ifarm at JLab.
- Basic info on how this can be done is [here](https://jlab.servicenowservices.com/scicomp?id=kb_article&sysparm_article=KB0015066)
- Suggestions on how to possibly streamline the process are [here](https://jlab.servicenowservices.com/scicomp?id=kb_article&sysparm_article=KB0014918)

## Environment set-up
If you are computing on the CUE system at JLab and using `tcsh` (or `csh`) as your shell, add the following lines to the `.tcshrc` or `.cshrc` file in your home directory:
```
source /group/halld/Software/build_scripts/gluex_env_boot_jlab.csh
gxenv
```
If you're using bash as your shell, the two lines above should also work.
(Note: I am using `tcsh`.  Some of what follows below will assume that you are also using `tcsh` or `csh`.  If you're a basher and encounter problems, keep in mind that the shell mismatch could be a cause.)

You can check the extent to which this worked by executing commands like
```
ccdb -i
root
hd_root
```
and making sure they don't crash.

## Using git

To clone this repo
> git clone https://github.com/mmccracken0137/gluex_analysis_example.git

Please contribute to this project!

### Data
#### 1. File types and the many things that we call them

The datasets generated by GlueX are BIG!  Most users do not interact with the full dataset, but rather generate skims relevant to their reaction(s) of interest.
The analysis process includes several major steps, some of which happen only rarely or upstream of the work that a data analyzer does:

- The "raw data" files (*i.e.*, information from GlueX's data acquisition system (DAQ)) are stored in EVIO (`.evio`) format ("event I/O").  The typical user will not interact with EVIO files.
- "Reconstruction" is then done on these files to produce REST ("REconstructed Showers and Tracks") files.  These files are too big for most users work with regularly, as they contain all tracks and showers for each event in each run period.  The reaction of interest to a user may only comprise a tiny fraction of each REST file. QQQ: Has particle ID been done at this stage?
- A skim is then done to skim out events from the REST files that are likely relevant to a user's analysis.  This process is automated.  The user requests a skim of a particular run period (*e.g.*, Spring 2017) for a particular reaction (*e.g.*, $$\gamma p \rightarrow K^{+}\Lambda \rightarrow K^{+} p \pi^{-}$$) using the [Analysis Reaction Submit form](https://halldweb.jlab.org/analysis/SubmitReaction.html).  The product of this skims are ROOT trees files, in which the reaction hypothesis has been applied to all events.

#### 2. Analysis launch
In order to generate a skim of the REST files, one uses the [Analysis Reaction Submit Form](https://halldweb.jlab.org/analysis/SubmitReaction.html).  This GUI allows the user to build the reaction using the drop-down particle menus.  An important part of skimming the data is performing a kinematic fit to the reaction hypothesis, and the user can specify features of this fit in the GUI.  Here are some important considerations:
- The "m" flag tells the event reconstruction that a particle is "missing", *i.e.* that it will not be "measured" in the event.  This could be a particle like a neutrino that simply does not interact with the detector and thus can't be tracked, or a particle that particle that did create a track/shower in the detector but which the user wishes to ignore.
- The "M" flag indicates that a particle's mass will not be constrained in the kinematic fit.  Constraining the mass hypothesis of a stable particle is often a good idea -- if you know that you want a proton in the final state, letting the fitter fix the particle's mass to the known proton mass will add important information to the fit.  Conversely, we often do not constrain the masses of particles that have significant width (*e.g.*, the $$\Delta$$ or hyperon resonances) or particles whose masses we don't yet know (are may be searching for).
- Fit type: TKTKTKTK

Your input in the GUI will format a text blurb that configures the analysis launch -- the rough draft of this text appears in the text box in the middle of the page.  When you click "Submit Reaction" the text is committed to the list of reactions that will be done when the next batch ("version") of skims is run.  The config text looks a bit cryptic, but the particle type codes in [this file](https://github.com/JeffersonLab/halld_recon/blob/master/src/libraries/include/particleType.h).

We're not going to submit an analysis launch at this time because the reaction we wish to analyze in this example has already been skimmed from the Spring 2017 Run Period.  The skim was configured with the following text:
```
Reaction1 1_14__11_18
Reaction1:Decay1 18__9_14
Reaction1:Flags F1_M18
```
The first line indicates that we're looking for $$\gamma p \rightarrow K^{+}\Lambda$$.
The second line indicates that we're looking for the $$\Lambda \rightarrow p \pi^-$$ decay.
The third line indicates that we will perform a kinematic fit to momentum only (`F1`) and the mass of the $$\Lambda$$ will not be constrained.

Looking at the [Spring 2017 Analysis Launch](https://halldweb.jlab.org/wiki-private/index.php/Spring_2017_Analysis_Launch) page, we see that this reaction was skimmed as part of version 69 (scoll down to "ver69").  You'll note that the text that configured our skim has been converted into a single string, `tree_kplamb__lamb_pimprot__M18_B4_F1`, which will be in the files associated with this skim.

Again, we're not submitting an analysis launch request at this time because our reaction has already been skimmed.

Lingo: The analysis launch runs a Reaction Filter on the REST files to produce "analysis trees".

#### 3. Moving files from tape to cache

OK, let's assume that the analysis launch batch has run and we've received an e-mail telling us that our skim files are ready.
These files are likely BIG, and the default behavior is for them to be stored on [JLab's Mass Storage System (MSS)](https://scicomp.jlab.org/docs/storage).
Our large, important data files are stored on tape because of tape's security and stability.
The `/mss` filesystem shows, but does not allow access to, the files that exist on the MSS.
We can list, but not access, these files from the ifarm:
```
> ls /mss/halld/RunPeriod-2017-01/analysis/ver69/tree_kplamb__lamb_pimprot__M18_B4_F1/merged/
tree_kplamb__lamb_pimprot__M18_B4_F1_030730.root  tree_kplamb__lamb_pimprot__M18_B4_F1_030744.root
tree_kplamb__lamb_pimprot__M18_B4_F1_030731.root  tree_kplamb__lamb_pimprot__M18_B4_F1_030745.root
tree_kplamb__lamb_pimprot__M18_B4_F1_030732.root  tree_kplamb__lamb_pimprot__M18_B4_F1_030749.root
tree_kplamb__lamb_pimprot__M18_B4_F1_030733.root  tree_kplamb__lamb_pimprot__M18_B4_F1_030754.root
tree_kplamb__lamb_pimprot__M18_B4_F1_030734.root  tree_kplamb__lamb_pimprot__M18_B4_F1_030769.root
tree_kplamb__lamb_pimprot__M18_B4_F1_030735.root  tree_kplamb__lamb_pimprot__M18_B4_F1_030770.root
tree_kplamb__lamb_pimprot__M18_B4_F1_030736.root  tree_kplamb__lamb_pimprot__M18_B4_F1_030778.root
tree_kplamb__lamb_pimprot__M18_B4_F1_030737.root  tree_kplamb__lamb_pimprot__M18_B4_F1_030779.root
tree_kplamb__lamb_pimprot__M18_B4_F1_030738.root  tree_kplamb__lamb_pimprot__M18_B4_F1_030780.root
tree_kplamb__lamb_pimprot__M18_B4_F1_030739.root  tree_kplamb__lamb_pimprot__M18_B4_F1_030783.root
tree_kplamb__lamb_pimprot__M18_B4_F1_030740.root  tree_kplamb__lamb_pimprot__M18_B4_F1_030784.root
tree_kplamb__lamb_pimprot__M18_B4_F1_030741.root  tree_kplamb__lamb_pimprot__M18_B4_F1_030785.root
tree_kplamb__lamb_pimprot__M18_B4_F1_030742.root  tree_kplamb__lamb_pimprot__M18_B4_F1_030787.root
tree_kplamb__lamb_pimprot__M18_B4_F1_030743.root  tree_kplamb__lamb_pimprot__M18_B4_F1_030788.root
```


Accessing data on tape is VERY inefficient.  As we tune our analysis, we may need to run our code over the data files many times -- doing so with tape is a bad idea.
So, the first step of working with our new tree files is to move them to a disk where we can access them more easily.
We do so with the `jcache` command.



The e-mail should tell us where to find these, but in gener

#### X. Setting up a working directory

We're now ready to start working!  We need a place to do this.  Brad Sawatzky's excellent [Jefferson Lab Scientific Computing Infrastructure Update](https://indico.jlab.org/event/863/contributions/14830/attachments/11384/17588/SciComp-Update-CLAS_June2024_Sawatzky.pdf) gives an overview of the computing resources at JLab.  Slide 10 of the presentation shows the various "Disk Storage Areas and Their Uses".

We're going to be writins some analysis code soon and we'd like it to be backed up -- looks like `/group` is a good fit.  So, let's create a directory on the space that's been allocated for Hall D work:
```
mkdir /group/halld/Users/mmccrack
```
Note that the default permission on this directory that I created are such that only I have write access, but all others can read the files in this directory.  That's good!  You often want to point other people to your code or files.

I'll also create a subdirectory for this specific analysis:
```
mkdir /group/halld/Users/mmccrack/gluex_KLambda_example/
```

QQQ: Where do we put large downstream data files that we've generated?

#### 4. Making and running a DSelector

Let's move to our working directory and get started:
```
cd /group/halld/Users/mmccrack/gluex_KLambda_example/
```

DSelectors are the primary way that users interact with the analysis tree files.
The [DSelector documentation](https://halldweb.jlab.org/doc-private/DocDB/ShowDocument?docid=4607) is quite good and is worth a read before you get too deep into your analysis work (maybe right after you read the simple instructions in this section).
Simply put, a DSelector is a piece of C code that will instruct ROOT how to parse the analysis tree files, make graphs of features of the events in the file, apply successive cuts to the data, and possible output smaller root tree files for downstream analysis.
One could in principle write a DSelector-type script from scratch, but the MakeDSelector executable makes a DSelector that will provide basic functionality specific to the reaction you used for your analysis skim.  Running `MakeDSelector` with no arguments tells us about the arguments needed by `MakeDSelector`:

```
> MakeDSelector

Makes a custom DSelector for the input TTree created by the DANA ANALYSIS library.
1st argument: The input ROOT TTree file name.
2nd argument: The name of the TTree in the input ROOT file that you want to make a selector for.
3rd argument: The base-name of the selector class & files you want to generate.
The generated files will be named "DSelector_<base-name>.C" and "DSelector_<base-name>.h".
```

In order to make an appropriate DSelector, `MakeDSelector` will actually open our analysis tree file (1st argument), look at the contents of the TTree we specify (2nd argument), and generate some code that we will run, edit, run, edit, run, edit, ...
We know our analysis tree file, but I can never remember how the TTree object name is formatted.
The good news is that we can take a look by loading the file in ROOT and listing its contents:
```
root [0] tf = TFile("/cache/halld/RunPeriod-2017-01/analysis/ver69/tree_kplamb__lamb_pimprot__M18_B4_F1/merged/tree_kplamb__lamb_pimprot__M18_B4_F1_030787.root")
(TFile &) Name: /cache/halld/RunPeriod-2017-01/analysis/ver69/tree_kplamb__lamb_pimprot__M18_B4_F1/merged/tree_kplamb__lamb_pimprot__M18_B4_F1_030787.root Title:
root [1] tf.ls()
TFile**		/cache/halld/RunPeriod-2017-01/analysis/ver69/tree_kplamb__lamb_pimprot__M18_B4_F1/merged/tree_kplamb__lamb_pimprot__M18_B4_F1_030787.root
 TFile*		/cache/halld/RunPeriod-2017-01/analysis/ver69/tree_kplamb__lamb_pimprot__M18_B4_F1/merged/tree_kplamb__lamb_pimprot__M18_B4_F1_030787.root
  KEY: TTree	kplamb__lamb_pimprot__M18_B4_F1_Tree;1	kplamb__lamb_pimprot__M18_B4_F1_Tree
```
The last line above is the TTree name that we want.
Great!  Let's now make our DSelector.  I'll use `kplam_analysis_ex' as the third argument for `MakeDSelector`:
```
> MakeDSelector /cache/halld/RunPeriod-2017-01/analysis/ver69/tree_kplamb__lamb_pimprot__M18_B4_F1/merged/tree_kplamb__lamb_pimprot__M18_B4_F1_030787.root kplamb__lamb_pimprot__M18_B4_F1_Tree kplam_analysis_ex
> ls -ltr
total 48
-rw-r--r--. 1 mmccrack halld  1983 Nov 21 11:34 DSelector_kplam_analysis_ex.h
-rw-r--r--. 1 mmccrack halld 22499 Nov 21 11:34 DSelector_kplam_analysis_ex.C
```

QQQ: In the comments of our made DSelector, PROOF is mentioned?  What is that?


#### 5. Running on the computing cluster
#### 6. Tuning cuts
#### 7. Creating and using plots

### Simulation



## Helpful links

- Brad Sawatzky, [Jefferson Lab Scientific Computing Infrastructure Update](https://indico.jlab.org/event/863/contributions/14830/attachments/11384/17588/SciComp-Update-CLAS_June2024_Sawatzky.pdf) (CLAS Collab Meeting, June 2024).  This presentation describes some of the features of SciComp at JLab, including which disks are appropriate for which uses.
- [AmpTools Starter Hints](https://halldweb.jlab.org/wiki-private/index.php/Amptools_starter_hints)
- [Naomi's personal link dump](https://halldweb.jlab.org/wiki-private/index.php/User:Njarvis)
- Brad Sawatzky, Basics of the JLab Computing Farm sessions from the [JLab GSPDA Mini Software Workshop](https://indico.jlab.org/event/879/) (6 Sep 2024)
- Naomi's [GoogleDoc about MCWrapper](https://docs.google.com/document/d/1m8ZWj1mVdu7c4xOZpdhYuzTFrJAwJzNtjgV9JQMDqzc/edit?tab=t.0)
- Alex Austregesilo, [Plug-ins, ReactionFilter and Analysis Submit Form](https://halldweb.jlab.org/DocDB/0040/004010/001/presentation.pdf) (May 2019)
- [How to choose software versions on the MC submit form](https://halldweb.jlab.org/wiki/index.php/How_to_choose_software_versions_on_the_MC_submission_form)
- For example, [Spring 2017 Analysis Launch](https://halldweb.jlab.org/wiki-private/index.php/Spring_2017_Analysis_Launch) information
- [GlueX Data Information](https://halldweb.jlab.org/wiki-private/index.php/GlueX_Data_Information)
- [MC Submit form](https://halldweb.jlab.org/gluex_sim/SubmitSim.html)
- [Private GlueX Wiki](https://halldweb.jlab.org/wiki-private/index.php/Main_Page)
- Analysis Launch [Reaction Submit form](https://halldweb.jlab.org/analysis/SubmitReaction.html)
- [DSelector info](https://halldweb.jlab.org/wiki/index.php/DSelector)
- David Abbott, [JLab Data Formats for DAQ](https://indico.bnl.gov/event/9458/contributions/43298/attachments/31408/49583/JLAB_SRO_DataFormat.pdf)




## Unorganized notes
Log into ifarm
Can pretty much get rid of everything in your .cshrc.  Environment is managed by gxenv.  Add the following lines to .cshrc:
> source /group/halld/Software/build_scripts/gluex_env_boot_jlab.csh
> gxenv

How to set up .cshrc at the lab and at CMU?



Glossary:
- exclusive -- reconstructing all final-state particles of the desired reaction
- inclusive -- not reconstructing all final-state particles of desired reaction.  Could possibly include some reactions other than the target.

Questions:
- What are the "versions" that accompany the different runs?  "Data versions"?  Why are there so many for each run period?
- Where does the data live?  How do I run over it?  Do I need to move it from tape every time I need to run over it?
- 


https://halldweb.jlab.org/data_monitoring/launch_analysis/index.html
What is each of the following things?
- Offline monitoring 
- Full Reconstruction Launch
- Analysis Launch
- Analysis Launch (Simulation)

Now we have to build some analysis code


Where are the data skims?  What do we actually run over?


Monte Carlo

With Naomi:
raw data is in /cache/halld files that should be there are shown /mss/halld/
evio are raw files
rest files are skims
RunPeriod/analysis/verXX
versions mean which batch a particular analysis launch skim is part of 
Look at run period 2017 -- small dataset, in process of being recalibrated.  2020 has good calibration.  Fall 2018 is enormous.

Write a DSelector to analyze the root tree files for a given reaction from analysis launch
be on gluon00 through 03

**How to get the files off of tape?**

use globus to transfer files from lab to CMU.  Broken at the moment.  globus.org

jcache get to move data from /mss to /cache
jcache -h for more info

For submitting Monte Carlo
https://halldweb.jlab.org/gluex_sim/SubmitSim.html
Run range looks at the flux for each run and generates the number of events in proportion to the flux of each run.  Number of events is total
Output directory is ONE directory.  MC Wrapper will put the files somewhere weird
Full path to generator -- put in /work/

EVIO is raw data
REST -- reconstructed showers and tracks
PART -- physics analysis ROOT tree, generated by running ReactionFilter on REST files

Write up analysis example.  Put it on the private wiki

Copy one datafile from jlab CMU to do prototyping.  
Need to loging to ernest then: sbatch <jobfile>